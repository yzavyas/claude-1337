# cognition

Cognitive offloading, trust, and metacognition research.

---

## cognitive offloading

Multiple 2024-2025 studies document measurable effects:

| study | finding | metric |
|-------|---------|--------|
| Gerlich 2025 | AI use correlates with critical thinking decline | r = -0.75 (strong) |
| Budzyń Lancet 2025 | Skill degradation in experienced professionals | 20% in 3 months |
| Kosmyna MIT 2025 | Users can't recall their own AI-assisted work | 83% failure |
| Lee CHI 2025 | AI confidence predicts less critical thinking | β = -0.69 |

**Gerlich (2025)**: Younger participants more affected. Higher education provides some mitigation.

**Budzyń et al. (2025)**: First clinical evidence. Experienced endoscopists showed adenoma detection rate drop from 28.4% to 22.4% after AI exposure.

**Kosmyna et al. (2025)**: Brain connectivity "systematically scaled down" across attention, working memory, and language regions.

---

## the over-reliance problem

**Computers in Human Behavior (2024).** Trust and reliance on AI.

| finding | risk |
|---------|------|
| Users over-rely to their **own detriment** | Personal harm |
| Mere "AI" label causes over-reliance | Labeling effect |
| Over-reliance persists despite **contradicting context** | Judgment override |
| Third parties affected | Broader harm |

---

## explainability paradox

**CHI 2021.** AI explanations and team performance.

| finding | implication |
|---------|-------------|
| Explanations **don't increase** accuracy | Transparency alone isn't enough |
| Explanations increase acceptance **regardless of correctness** | Can cause over-reliance |

**Scientific Reports (2024)** found explanations help when they enable **validation**, not just acceptance.

**Resolution:** Explanations work when users can validate against domain knowledge.

---

## metacognition

**Fernandes et al. (2025)**: AI improved logical reasoning (~3 points on LSAT) but participants overestimated performance by 4 points. Higher AI literacy correlated with *worse* calibration.

**Ma et al. CHI (2024)**: Calibration status feedback and "think the opposite" both improved self-confidence calibration. But effective calibration increases cognitive load.

**Fan et al. (2024)**: ChatGPT improved task performance but reduced metacognitive activities. Termed this "metacognitive laziness."

| finding | source | implication |
|---------|--------|-------------|
| AI literacy → worse calibration | Fernandes 2025 | Literacy education alone insufficient |
| Calibration feedback helps | Ma CHI 2024 | But increases cognitive load |
| Metacognitive laziness | Fan BJET 2024 | AI use reduces planning/monitoring |

---

## design implications

| principle | implementation |
|-----------|----------------|
| Calibrated trust | Acknowledge skill limitations |
| Validation-enabling | Show reasoning, not just conclusions |
| Critical engagement | Prompt evaluation, not acceptance |
| Appropriate reliance | Clear "use when" boundaries |
| Metacognitive support | Frameworks for self-evaluation |

**Core principle:** Skills should enable validation, not demand trust.
