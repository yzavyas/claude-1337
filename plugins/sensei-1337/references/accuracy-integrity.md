# Accuracy and Intellectual Integrity

Teaching effectiveness means nothing if the content is wrong.

---

## The Problem

Effective teaching techniques can spread misinformation faster. Persuasive framing, emotional resonance, memorable structure — all amplify reach. If what you're teaching is wrong, you've made the problem worse.

---

## Why Accuracy Matters

### Ethical dimension

Teaching creates an asymmetric trust relationship. The learner is vulnerable:
- They don't know what they don't know
- They trust you to represent information fairly
- They'll make decisions based on what you teach
- They'll pass it on to others

Misrepresentation exploits that trust.

### Material dimension

Wrong information has real costs:

| Cost | Mechanism |
|------|-----------|
| **Compounding error** | Wrong foundations → wrong conclusions → wrong decisions |
| **Unlearning difficulty** | First impressions persist; correction requires more effort than initial learning |
| **Credibility collapse** | When errors discovered, everything you taught becomes suspect |
| **Downstream spread** | Learners cite, build on, and pass on what you taught |

---

## Common Failure Modes

### 1. Inference presented as finding

**The pattern**: Study measures X. Teacher says "study shows Y" where Y is an inference from X.

**Example**:
- Study: "Strong negative correlation (r = -0.75) between AI use and critical thinking scores"
- Wrong: "AI use causes critical thinking decline"
- Right: "AI use correlates with lower critical thinking scores"

Correlation ≠ causation. Mechanism ≠ observation.

### 2. Selective citation

**The pattern**: Citing evidence that supports your position while ignoring contradictory evidence.

**Example**:
- Citing three studies showing AI improves productivity
- Ignoring the meta-analysis showing mixed results

The honest approach: acknowledge the full evidence landscape, including uncertainty.

### 3. Mechanism invention

**The pattern**: Explaining *why* something works when the mechanism is actually unknown.

**Example**:
- Study: "Transparency features correlate with better outcomes"
- Wrong: "Transparency works because users feel more in control"
- Right: "Transparency correlates with better outcomes; the mechanism is unclear"

Don't invent explanations for observed correlations.

### 4. Implication overreach

**The pattern**: Drawing conclusions the evidence doesn't actually support.

**Example**:
- Finding: "Design features X, Y, Z showed strong effects in two studies"
- Wrong: "AI collaboration enhances rather than hollows when designed right"
- Right: "These design features correlated with better outcomes in these contexts"

State what was found. Label inferences as inferences.

### 5. Framing bias

**The pattern**: Presenting values or philosophy as if they were empirical findings.

**Example**:
- Wrong: "This isn't philosophy — it's evidence-based"
- Right: "This is our philosophy, informed by evidence"

Values guide which evidence matters and how to interpret it. Acknowledge them.

---

## The Accuracy Test

Before teaching anything, ask:

### 1. What was actually found?

Not what you infer. What was measured, in what population, with what methodology?

### 2. What's the evidence quality?

| Factor | Questions |
|--------|-----------|
| Sample | Size? Representative? |
| Design | Observational or experimental? Controls? |
| Replication | Has this been replicated? By whom? |
| Effect size | How large is the effect? Practically significant? |

### 3. What are you adding?

Distinguish clearly between:
- What the source claims
- What you infer from it
- What you believe independent of evidence

### 4. Would an honest skeptic accept this framing?

If someone who disagreed with your conclusion read your presentation, would they say you represented the evidence fairly?

If not, revise.

---

## Reasoning Verification Techniques

Knowing failure modes isn't enough. You need techniques to catch them in your own reasoning.

### Chain of Verification (CoVe)

Generate verification questions, answer them independently, then revise.

| Step | Action | Example |
|------|--------|---------|
| 1. Initial claim | State what you're about to teach | "Transparency improves learning" |
| 2. Generate verification Qs | Ask questions that could falsify | "What was measured?" "Correlation or causation?" "Effect size?" "Contradictory findings?" |
| 3. Answer independently | Answer each without defending the claim | "Task completion, not learning" "Correlational" "d=0.34" "One null result" |
| 4. Revise | Update claim based on answers | "Transparency correlated with better task completion (d=0.34) in some studies" |

**Why it works**: Generating verification questions activates critical thinking. Answering independently prevents motivated reasoning.

### Decomposition

Break complex claims into atomic sub-claims. Verify each.

| Complex claim | Decomposed | Verification |
|---------------|------------|--------------|
| "AI collaboration enhances capability" | 1. "AI + human outperforms human alone" | Check: In what tasks? What metrics? |
| | 2. "The human learns from the collaboration" | Check: Was learning measured? Or just performance? |
| | 3. "Enhancement persists after AI removed" | Check: Was this tested? |

**Why it works**: Complex claims hide weak links. Decomposition exposes them.

### Adversarial Check

Steel-man the counter-argument. Does your claim survive?

| Your claim | Steel-manned counter | Verdict |
|------------|---------------------|---------|
| "Effect size d=0.86 is large" | "In education research, d=0.4 is typical. What's the practical significance?" | Add context about typical effect sizes |
| "Study shows X works" | "N=16, single context, no replication. How generalizable?" | Acknowledge limitations |

**Why it works**: If you can't defeat the strongest counter-argument, your claim is weaker than you thought.

### Source Triangulation

Multiple independent sources > single source.

| Evidence level | Description |
|----------------|-------------|
| **Strong** | Multiple meta-analyses, independent replications |
| **Moderate** | Several studies, some replication |
| **Weak** | Single study, no replication |
| **Speculative** | Theoretical only, no empirical support |

**Label accordingly**: "Meta-analysis of 55 studies shows..." vs "One study suggests..."

### The Quick Check

Before teaching any claim, run through:

1. **What was actually measured?** (not inferred)
2. **Correlation or causation?** (don't upgrade)
3. **Effect size and sample?** (context for significance)
4. **Replicated?** (single study = tentative)
5. **Counter-evidence?** (acknowledge it)

If you can't answer these, you're not ready to teach the claim.

---

## Practical Guidelines

| Situation | Do | Don't |
|-----------|----|----|
| Citing research | State what was measured | Say "proves" or "shows" for correlations |
| Drawing implications | Label as "suggests" or "we interpret as" | Present inference as finding |
| Acknowledging limits | State sample size, context, methodology | Cherry-pick favorable framings |
| Philosophy/values | Label explicitly as values | Dress as empirical claims |

---

## The Stakes

Teaching wrong things effectively is worse than not teaching at all.

An ineffective teacher of truth does less harm than an effective teacher of falsehood. The skills in this guide — psychology, audience empathy, rhetoric, structure — are amplifiers. They make whatever you're teaching land harder.

Use them on accurate content.
