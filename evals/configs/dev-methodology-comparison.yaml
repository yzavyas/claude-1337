# Development Methodology Comparison Experiment
#
# Compares spec-driven development methodologies:
# - Baseline: Pure Claude, no methodology
# - spec-kit: GitHub's 7-step process (Constitution→Implement)
# - GSD: Context engineering with subagent isolation
# - BMAD: Scale-adaptive agile with specialized agents
#
# Run with: evals-1337 compare configs/dev-methodology-comparison.yaml
#
# Paths are relative to this file's directory (evals/configs/)

name: dev-methodology-experiment-v1
description: |
  Compare spec-driven development methodologies on real coding tasks.

  Research questions:
  - Does structured spec-driven development improve outcomes?
  - Which methodology works best for which task type?
  - What's the overhead vs quality tradeoff?

configs:
  # Control: Pure Claude with no methodology
  baseline:
    description: "Pure Claude, no methodology loaded"
    system_prompt: null
    claude_md_path: null
    skill_paths: []

  # Treatment 1: GitHub spec-kit
  speckit:
    description: "GitHub spec-kit: Constitution→Specify→Plan→Tasks→Implement"
    system_prompt: null
    claude_md_path: null
    skill_paths:
      - "../methodologies/speckit/SKILL.md"

  # Treatment 2: Get Shit Done
  gsd:
    description: "GSD: PROJECT/ROADMAP/STATE/PLAN with subagent isolation"
    system_prompt: null
    claude_md_path: null
    skill_paths:
      - "../methodologies/gsd/SKILL.md"

  # Treatment 3: BMAD Method
  bmad:
    description: "BMAD: 21 agents, scale-adaptive L0-L4, 34 workflows"
    system_prompt: null
    claude_md_path: null
    skill_paths:
      - "../methodologies/bmad/SKILL.md"

# Experiment parameters
experiment:
  runs_per_case: 5  # Handle stochasticity
  model: "claude-sonnet-4-20250514"
  max_tokens: 8192

  # Metrics to collect
  metrics:
    # Primary: Task completion (ground truth)
    - name: task_completion
      type: code
      description: "Test suite pass/fail"

    # Secondary: Quality (LLM-as-judge)
    - name: code_quality
      type: llm_judge
      rubric_path: "../rubrics/code-quality.yaml"

    # Secondary: Spec quality (methodology-specific)
    - name: spec_quality
      type: llm_judge
      applies_to: [speckit, gsd, bmad]
      rubric_path: "../rubrics/spec-quality.yaml"

    # Efficiency metrics
    - name: tokens_used
      type: counter
    - name: time_to_complete
      type: timer
    - name: iterations
      type: counter

# Test corpus reference
test_corpus: "../suites/dev-methodology-v1.json"

# Analysis configuration
analysis:
  # Stratify results by task category
  stratify_by: ["category", "difficulty"]

  # Statistical tests
  tests:
    - name: "overall_f1_comparison"
      type: "anova"
      groups: ["baseline", "speckit", "gsd", "bmad"]

    - name: "pairwise_improvement"
      type: "paired_t_test"
      baseline: "baseline"
      treatments: ["speckit", "gsd", "bmad"]

  # Effect size thresholds
  thresholds:
    small: 0.2
    medium: 0.5
    large: 0.8

# Hypotheses (pre-registered)
hypotheses:
  H1:
    name: "Spec overhead tradeoff"
    prediction: "Heavier methods have lower pass@1 but higher code quality"
    metrics: ["pass_at_1", "code_quality"]

  H2:
    name: "Complexity scaling"
    prediction: "Lightweight methods degrade on complex tasks"
    metrics: ["pass_at_k"]
    stratify: "difficulty"

  H3:
    name: "Reliability"
    prediction: "Structured methods have higher pass^k (consistency)"
    metrics: ["pass_k"]

  H4:
    name: "Recovery"
    prediction: "Spec-driven methods recover better from failures"
    metrics: ["recovery_rate"]

  H5:
    name: "Efficiency frontier"
    prediction: "Diminishing returns on token cost vs quality"
    metrics: ["tokens_used", "code_quality"]
