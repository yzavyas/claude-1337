# Code Quality Rubric for LLM-as-Judge Evaluation
#
# Used to score code produced by different methodologies.
# Judge should be blind to which methodology produced the code.

name: code-quality
version: "1.0.0"
description: |
  Rubric for evaluating code quality independent of task completion.
  A solution can pass tests but have poor quality (brittle, unreadable, etc.)

judge_model: "claude-sonnet-4-20250514"
judge_temperature: 0.0

# System prompt for the judge
judge_system_prompt: |
  You are evaluating code quality. Score each criterion 0-1.
  Be objective and consistent. Do not consider whether the code "works" -
  that's measured separately. Focus only on quality attributes.

criteria:
  - name: correctness_approach
    weight: 0.25
    description: "Is the approach fundamentally sound?"
    scoring:
      0.0: "Fundamentally flawed approach that will fail edge cases"
      0.5: "Reasonable approach with some gaps"
      1.0: "Sound approach that handles requirements comprehensively"
    prompt: |
      Does this code take a fundamentally sound approach to solving the problem?
      Consider: algorithm choice, data structure selection, error handling strategy.
      Score 0-1.

  - name: readability
    weight: 0.20
    description: "Is the code easy to understand?"
    scoring:
      0.0: "Confusing, poorly named, hard to follow"
      0.5: "Understandable but could be clearer"
      1.0: "Clear, well-named, easy to follow"
    prompt: |
      Is this code readable and understandable?
      Consider: variable names, function names, code organization, complexity.
      Score 0-1.

  - name: maintainability
    weight: 0.20
    description: "Is the code easy to modify and extend?"
    scoring:
      0.0: "Tightly coupled, hard to change, brittle"
      0.5: "Somewhat modular but with coupling issues"
      1.0: "Well-structured, loosely coupled, easy to modify"
    prompt: |
      Is this code maintainable?
      Consider: separation of concerns, coupling, cohesion, DRY principle.
      Score 0-1.

  - name: error_handling
    weight: 0.20
    description: "Are errors handled appropriately?"
    scoring:
      0.0: "No error handling or swallows errors silently"
      0.5: "Basic error handling but misses cases"
      1.0: "Comprehensive error handling with informative messages"
    prompt: |
      Does this code handle errors appropriately?
      Consider: input validation, exception handling, error messages, edge cases.
      Score 0-1.

  - name: security
    weight: 0.15
    description: "Are there security concerns?"
    scoring:
      0.0: "Obvious vulnerabilities (injection, XSS, etc.)"
      0.5: "Some security awareness but gaps"
      1.0: "No obvious vulnerabilities, follows security best practices"
    prompt: |
      Is this code secure?
      Consider: input sanitization, injection risks, authentication/authorization, data exposure.
      Score 0-1.

# Aggregation
aggregation: weighted_average

# Thresholds for interpretation
thresholds:
  excellent: 0.85
  good: 0.70
  acceptable: 0.50
  poor: 0.0

# Judge prompt template
judge_prompt_template: |
  Evaluate the following code for quality.

  ## Task Description
  {task_prompt}

  ## Code Produced
  ```
  {code}
  ```

  ## Evaluation Criteria
  Score each criterion 0.0 to 1.0:

  1. **Correctness of Approach** (weight: 25%): Is the approach sound?
  2. **Readability** (weight: 20%): Is it easy to understand?
  3. **Maintainability** (weight: 20%): Is it easy to modify?
  4. **Error Handling** (weight: 20%): Are errors handled well?
  5. **Security** (weight: 15%): Are there vulnerabilities?

  Respond with JSON:
  ```json
  {
    "correctness_approach": 0.X,
    "readability": 0.X,
    "maintainability": 0.X,
    "error_handling": 0.X,
    "security": 0.X,
    "reasoning": "Brief explanation of scores"
  }
  ```
