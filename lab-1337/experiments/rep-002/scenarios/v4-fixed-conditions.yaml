# REP-002 v4: Fixed Conditions
#
# Tests the ACTUAL hypothesis: Does NOVEL context (WHAT + WHY + CONSTRAINTS)
# produce better results than spec-driven processes or pure baseline?
#
# v3 flaw: "principle-guided" had REDUNDANT info Claude already knows
# v4 fix: "what-why-constraints" has NOVEL domain context
#
# Run: lab-1337 batch run v4-fixed-conditions -e rep-002 --grader function
# Judge: lab-1337 judge evaluate rep-002-v4-fixed-conditions -e rep-002
#
name: rep-002-v4-fixed-conditions

grader: function

tasks:
  - tasks/discriminating/safe-calculator-task.yaml

conditions:
  # Baseline: Minimal framing, pure Claude judgment
  - conditions/full-autonomy.md

  # Core-1337 style: NOVEL context (WHY this matters, domain constraints)
  - conditions/what-why-constraints.md

  # Spec-driven: Explicit process (GSD/BMAD/SpecKit methodology)
  - conditions/spec-driven.md

# Sonnet for better capability
model: sonnet

# n=10 per condition (30 total runs) for signal detection
runs_per_condition: 10

iteration:
  strategy: self-review
  max_iterations: 2
  review_prompt: |
    Review your implementation:
    1. Does it handle all the requirements correctly?
    2. Are there edge cases you haven't considered?
    3. Is it robust against unexpected input?

# Expected: ~30 runs × ~2-3 min = ~60-90 minutes
# Cost: ~30 runs × $0.15-0.20 = ~$5-6
