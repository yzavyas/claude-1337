---
import Base from '../layouts/Base.astro';
---

<Base title="explanation" description="why skills don't activate and how we fixed it">
  <p style="color: var(--fg-dim);">understanding the skill activation problem</p>

  <h2>the problem</h2>

  <p>claude code skills have a ~20% activation rate by default. you install a skill, ask a relevant question, and claude ignores it.</p>

  <p>this isn't a bug. it's emergent behavior from how skills are surfaced.</p>

  <h2>how skills work</h2>

  <p>skills appear in an <code>&lt;available_skills&gt;</code> block in claude's context:</p>

  <pre><code>&lt;available_skills&gt;
&lt;skill&gt;
  &lt;name&gt;terminal-1337&lt;/name&gt;
  &lt;description&gt;modern cli tools...&lt;/description&gt;
&lt;/skill&gt;
...
&lt;/available_skills&gt;</code></pre>

  <p>claude sees this but doesn't automatically:</p>

  <ol>
    <li>evaluate each skill against the current request</li>
    <li>decide which skills are relevant</li>
    <li>activate them before responding</li>
  </ol>

  <p>it just... responds. sometimes it notices skills, usually it doesn't.</p>

  <h2>the research</h2>

  <p>scott spence documented this problem and found a fix:</p>

  <blockquote style="border-left: 2px solid var(--accent); padding-left: 1rem; margin: 1rem 0;">
    <p>"skills don't trigger automatically - claude needs explicit instruction to evaluate them"</p>
  </blockquote>

  <p>his forced evaluation prompt achieved 84% activation (4.2x improvement).</p>

  <h2>our eval framework</h2>

  <p>we built an eval framework using claude agent sdk to measure activation rigorously:</p>

  <pre><code># rigorous evaluation uses labeled test cases
suite = TestSuite(
    skills=[
        SkillTestSpec(
            name="rust-1337",
            test_cases=[
                # should activate
                TestCase(prompt="what crate for cli args?", expectation="must_activate"),
                # should NOT activate
                TestCase(prompt="help me write python", expectation="should_not_activate"),
                # ambiguous
                TestCase(prompt="explain ownership", expectation="acceptable"),
            ]
        )
    ],
    negative_cases=[
        TestCase(prompt="write a haiku", expectation="should_not_activate"),
    ]
)

# measures precision (avoid false activations) AND recall (catch valid triggers)
report = await run_test_suite(suite, mode="baseline")
print(f"Precision: {report.precision}, Recall: {report.recall}, F1: {report.f1}")</code></pre>

  <p><strong>important:</strong> raw activation rate is meaningless without measuring false positives. a system that activates on every prompt has 100% recall but 0% precision.</p>

  <h2>the fix</h2>

  <p>add this to your system prompt:</p>

  <pre><code>When you receive a request that might benefit from specialized knowledge:

1. Check if you've already activated a relevant skill this session
2. If not, scan &lt;available_skills&gt; for matches and activate before responding
3. Skip re-evaluation for topics you've already covered</code></pre>

  <h2>why this works</h2>

  <p>the prompt forces explicit evaluation before response generation. claude checks skills once per topic rather than ignoring them entirely.</p>

  <p>this is similar to chain-of-thought prompting â€” forcing the "should I use a skill?" step prevents Claude from skipping straight to answering.</p>

  <h2>testing vs production</h2>

  <p>for testing activation rates, use the aggressive version:</p>

  <pre><code>Before responding to ANY request, you MUST evaluate skills...</code></pre>

  <p>for production, the per-topic version saves tokens while maintaining activation.</p>

  <h2>tradeoffs</h2>

  <table>
    <tr>
      <th>aspect</th>
      <th>baseline</th>
      <th>smart eval</th>
      <th>forced eval</th>
    </tr>
    <tr>
      <td>recall (catches triggers)</td>
      <td class="bad">~20%</td>
      <td class="good">~70%</td>
      <td class="good">~84%</td>
    </tr>
    <tr>
      <td>precision (avoids noise)</td>
      <td>high</td>
      <td>medium</td>
      <td class="bad">lower</td>
    </tr>
    <tr>
      <td>token usage</td>
      <td>baseline</td>
      <td>+once per topic</td>
      <td>+every message</td>
    </tr>
  </table>

  <p><strong>recommendation:</strong> use "smart eval" (per-topic) for production. forced eval inflates recall but may increase false activations.</p>

  <h2>open questions</h2>

  <ul>
    <li>does anthropic plan to improve default activation?</li>
    <li>can skills self-trigger based on keywords?</li>
    <li>what's the optimal number of skills before truncation?</li>
  </ul>

  <h2>sources</h2>

  <ul>
    <li><a href="https://scottspence.com/posts/claude-code-skills">scott spence - claude code skills research</a></li>
    <li><a href="https://www.reddit.com/r/ClaudeAI/comments/1pha74t/deep_dive_anatomy_of_a_skill_its_tokenomics_why/">reddit - anatomy of a skill, tokenomics, truncation</a></li>
    <li><a href="https://www.reddit.com/r/ClaudeAI/comments/1pe37e3/claudemd_and_skills_experiment_whats_the_best_way/">reddit - CLAUDE.md and skills experiment</a></li>
    <li>our eval framework: <code>/evals</code> in the repo</li>
  </ul>
</Base>
