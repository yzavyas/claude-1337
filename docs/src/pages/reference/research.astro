---
import Base from '../../layouts/Base.astro';
---

<Base title="research" description="skill activation research and validation framework">
  <p><a href="/claude-1337/reference/">&larr; reference</a></p>

  <h1>research</h1>

  <p class="dimmed-intro">skill activation studies and the claude-1337 validation framework</p>

  <h2>the activation problem</h2>

  <p>claude code skills have a ~20% baseline activation rate. you install a skill, ask a relevant question, and claude ignores it 80% of the time.</p>

  <h3>root cause</h3>

  <p>from <a href="https://leehanchung.github.io/blogs/2025/10/26/claude-skills-deep-dive/">lee han chung's deep dive</a>:</p>

  <ul>
    <li><strong>no algorithmic routing</strong> — no regex, no embeddings, no classifiers</li>
    <li><strong>pure LLM reasoning</strong> — claude reads skill descriptions and decides</li>
    <li><strong>description is everything</strong> — the only signal for matching</li>
  </ul>

  <p>problem: claude sees the skills but doesn't automatically evaluate them against your request. it responds without checking if a skill would help.</p>

  <h2>the forced evaluation study</h2>

  <p>from <a href="https://scottspence.com/posts/how-to-make-claude-code-skills-activate-reliably">scott spence's 200+ test study</a>:</p>

  <table>
    <thead>
      <tr>
        <th>approach</th>
        <th>activation rate</th>
        <th>notes</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>no intervention (baseline)</td>
        <td>~20%</td>
        <td>default behavior</td>
      </tr>
      <tr>
        <td>simple instruction</td>
        <td>~20%</td>
        <td>doesn't help</td>
      </tr>
      <tr>
        <td>LLM eval hook</td>
        <td>80%</td>
        <td>asks claude to evaluate</td>
      </tr>
      <tr>
        <td>forced eval hook</td>
        <td><strong>84%</strong></td>
        <td>explicit skill checking</td>
      </tr>
    </tbody>
  </table>

  <h3>the fix</h3>

  <p>explicit evaluation prompts that force claude to check skills before responding. this is what core-1337's SessionStart hook implements.</p>

  <pre><code>Before responding:
1. Check if any skills in &lt;available_skills&gt; are relevant
2. If relevant, invoke the Skill tool
3. Then respond using that knowledge</code></pre>

  <h2>validation framework</h2>

  <p>the claude-1337 eval framework validates activation rates by observing actual tool invocation, not asking claude's opinion.</p>

  <h3>methodology</h3>

  <p>tests send prompts through the claude agent sdk and monitor the response stream for <code>ToolUseBlock</code> with <code>name == "Skill"</code>. this is ground truth — did claude actually invoke the skill?</p>

  <pre><code>async for message in query(prompt=prompt, options=options):
    if isinstance(message, AssistantMessage):
        for block in message.content:
            if isinstance(block, ToolUseBlock):
                if block.name == "Skill":
                    skill_called = True  # ground truth</code></pre>

  <h3>test suite format</h3>

  <pre><code>{
  "name": "claude-1337-skills",
  "description": "test activation of marketplace skills",
  "skills": [
    {
      "name": "terminal-1337",
      "plugin": "terminal-1337",
      "expected_triggers": [
        "how do i search for a pattern in my codebase?",
        "what's a fast way to find files by name?"
      ]
    }
  ],
  "runs_per_prompt": 3
}</code></pre>

  <h3>interpreting results</h3>

  <table>
    <thead>
      <tr>
        <th>activation rate</th>
        <th>meaning</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>80%+</td>
        <td>skill description is working well</td>
      </tr>
      <tr>
        <td>50-79%</td>
        <td>description needs improvement</td>
      </tr>
      <tr>
        <td>&lt;50%</td>
        <td>description likely missing "use when:" or too vague</td>
      </tr>
    </tbody>
  </table>

  <h2>running tests</h2>

  <h3>installation</h3>

  <pre><code>cd evals
uv sync</code></pre>

  <p><em>note: tested with claude code subscription (max plan). api testing status documented in <a href="https://github.com/yzavyas/claude-1337/tree/main/evals">evals/README.md</a></em></p>

  <h3>single test</h3>

  <pre><code>uv run skill-test test "how do i search for a pattern?" -s terminal-1337 -n 3</code></pre>

  <h3>run test suite</h3>

  <pre><code># create sample suite
uv run skill-test init-suite sample-suite.json

# run suite
uv run skill-test suite sample-suite.json -o report.md</code></pre>

  <h2>what makes skills activate</h2>

  <p>patterns that improve activation rates:</p>

  <table>
    <thead>
      <tr>
        <th>pattern</th>
        <th>why it works</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>"use when:" clause</td>
        <td>explicit trigger conditions</td>
      </tr>
      <tr>
        <td>specific tools/terms</td>
        <td>"axum, tonic, sqlx" not "backend"</td>
      </tr>
      <tr>
        <td>action verbs</td>
        <td>"building", "debugging", "configuring"</td>
      </tr>
      <tr>
        <td>front-loaded keywords</td>
        <td>claude matches against description</td>
      </tr>
    </tbody>
  </table>

  <h2>sources</h2>

  <ul>
    <li><a href="https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills">anthropic: equipping agents with skills</a> — official documentation</li>
    <li><a href="https://scottspence.com/posts/how-to-make-claude-code-skills-activate-reliably">scott spence: skills activation study</a> — 200+ test validation of forced eval pattern</li>
    <li><a href="https://leehanchung.github.io/blogs/2025/10/26/claude-skills-deep-dive/">lee han chung: skills deep dive</a> — how skill routing actually works</li>
    <li><a href="https://www.reddit.com/r/ClaudeAI/comments/1pha74t/deep_dive_anatomy_of_a_skill_its_tokenomics_why/">deep dive: anatomy of a skill</a> — tokenomics and available_skills budget</li>
    <li><a href="https://www.reddit.com/r/ClaudeAI/comments/1pe37e3/claudemd_and_skills_experiment_whats_the_best_way/">CLAUDE.md and skills experiment</a> — optimal knowledge distribution patterns</li>
  </ul>

  <h2>structure</h2>

  <pre><code>evals/
├── .env.example              # api key template
├── main.py                   # cli entry point
├── pyproject.toml            # uv project config
├── README.md                 # detailed documentation
├── sample-suite.json         # example test suite
├── src/
│   ├── cli.py               # command interface
│   ├── runner.py            # test execution
│   └── report.py            # markdown generation
└── uv.lock                  # dependency lock</code></pre>

  <p><a href="https://github.com/yzavyas/claude-1337/tree/main/evals">view evals/ on github</a></p>
</Base>
